---
title: "XGBoost Tree vs. Linear"
author: "Fabian Müller"
date: "17 12 2017"
output: html_document
references:
- id: chen2016
  title: XGBoost - A Scalable Tree Boosting System
  author:
  - family: Chen
    given: Tianqi
  - family: Guestrin
    given: Carlos
  URL: 'https://arxiv.org/pdf/1603.02754.pdf'
  type: article-journal
  issued:
    year: 2016
- id: Natekin2013
  title: Gradient boosting machines, a tutorial
  author:
  - family: Natekin
    given: Alexey
  - family: Knoll
    given: Alois
  URL: 'https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full'
  type: article-journal
  issued:
    year: 2013
      
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
library(dplyr)

load(file = "../02_results/sim_results_summary.RData")
```

## Introduction

One of the highlights of this year's [H2O World](https://h2oworld.h2o.ai) was a [Kaggle Grandmaster Panel](https://www.youtube.com/watch?feature=youtu.be&v=bFHRmesTCc0&app=desktop). The attendees, [Gilberto Titericz](https://www.kaggle.com/titericz) (Airbnb), Mathias Müller (H2O.ai), [Dmitry Larko](https://www.kaggle.com/dmitrylarko) (H2O.ai), Marios Michailidis (H2O.ai), and Mark Landry (H2O.ai), answered various questions about Kaggle and data science in general. 

One of the questions from the audience was which tools and algorithms the Grandmasters frequently use. As expected, every single of them named the gradient boosting implementation XGBoost [@chen2016]. This is not suprising, since it is long known that XGBoost is at the moment the probably most used algorithm in data science.  

XGBoost popularity manifests itself in various blog posts. Including tutorials for [R](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial) and [Python](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn), [Hyperparameter for XGBoost](https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html), and even [using XGBoost with Nvidia's CUDA GPU support](https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda).

At STATWORX, we also frequently leverage XGBoost's power for external and internal projects. One question that we lately asked ourself was 
how big the difference between the two **base learners** (also called *boosters*) offered by XGBoost is? This posts tries to answer this question in a more systematic way.


## Weak Learner

Gradient boosting can be interpreted as a combination of single models (so called *base learner* or *weak learner*) to an ensemble model (@Natekin2013).
In theory, any base learner can be used in the boosting framework, whereas some base learners have proven themself to be particularly useful: 
Linear and penalized models (Hastie et al. 2008), (p/b-) splines (Huang/Yang 2004), and especially decision trees (James et al. 2013). Among the less frequently used base learners are random effects (Tutz/Groll 2009), radial basis functions (Gomez-Verdejo et al. 2002), markov random fields (Dietterich et al. 2004) und wavlets (Dubossarsky et al. 2016).

Chen and Guestrin (2016) describe XGBoost as an additive function, given the data $D = \{(y_i, x_i)\}$, of the following form: 

$$\hat{y_i} = \phi(x_i) = \sum^{K}_{k=1} f_k(x_i),\textrm{  } f_k\in F$$

In their original paper, $f_k(x) \forall k = 1, ...K$ is defined as an *classification or regression tree* (CART). Apart from that, the alert reader of the technical documentation knows that one can alter the functional form of $f_k(x)$ by using the `booster` argument in R/Python:

```{r, eval=FALSE}
# Example of XGBoost for regression in R with trees (CART)
xgboost(data = train_DMatrix,
        obj = "reg:linear".
        eval_metric = "rmse",
        booster = "gbtree")
```

One can choice between decision trees (`gbtree` und `dart`) and linear models (`gblinear`). Unfortunately, there exists only a limited literature on the comparision of different base learners for boosting (see for example Joshi et al. 2002). For the special case of XGBoost, there is to our knowledge not systematic comparision available. 


## Simulation and Setup

In order to compare linear with trees base learners, we propose the following Monte Carlo simulation:

1) Draw a random number $n$ from a uniform distrubtion $[100, 2500]$.   
2) Simulate four datasets, two for classification and two for regression, each having $n$ observations. 
3) On each dataset, train a boosting model with tree and linear base learners, respectively.   
4) Calculate an appropriate error metric for each model on each dataset.   

Repeat the outlined procedure $m = 100$ times. 

As for simulation, we use the functions `twoClassSim()`, `LPH07_1()`, `LPH07_2()`, `SLC14_1()` from the `caret` package. In addition to the relevant features, a variying number of (correlated) random features was added. Note that in order to match real life data, all data generating processes involve non-linear components. For further details, we advise the reader to take a look at the `caret` package documentation.

For each datasets, we apply the same (random) splitting strategies where 70% of the data goes to training, 15% is used for validation, and the last 15% is used for testing. Regarding hyperparameter tuning, we use a grid-search strategy in combindation with 10-fold crossvalidation on the training data. Regardless of the base learner type, $L1$ (`alpha`) and $L2$ (`lambda`) regularization were tuned using a shared parameter space.
For tree boosting, the learning rate (`eta`) was held constant at 0.3 while tuning the optimal tree size (`max_depth`). Finally, we used a fixed number of 1000 boosting iterations (`nrounds`) in combination with ten early stopping rounds (`early_stopping_rounds`) on the validation frame. The final performance was evaluated by appliying the model with the best crossvalidated parameters on the test dataset. 


## Results

Figure 1 and Figure 2 show the distributions of out of sample classification errors (AUC) and regression errors (RMSE) for both datasets. Associated summary statistics can be found in Table 1.

```{r, echo=FALSE}
sim_results_summary %>% 
  ungroup() %>% 
  mutate(type = ifelse(data_set <= 2, "Classification", "Regression"),
         error = ifelse(data_set <= 2, "AUC", "RMSE"),
         booster = ifelse(booster == "linear", "Linear", "Tree")) %>% 
  dplyr::select(booster, data_set, type, error, metric_mean, metric_sd) %>% 
  knitr::kable(., 
               col.names = c("Base learner", "Dataset", "Type", "Error metric", "Average error", "Error std."),
               caption = "Table 1: Error summary statistics by datasets and base learners")
```


For the first dataset, the models using tree learners are on average better than the models with linear learners. However, the tree models exhibit a greater variance. The relationships are reversed for the second dataset. On average, the linear models are slightly better and the tree models exhibit a lower variance.

<center>![](plots/result_oos_classification.png){ width=90%}</center>

In contrast to the classification case, there is for both regression datasets a substential difference in performance in favor of the tree models. For the third dataset, the tree models are on average better than their linear counterparts. Also, the variance of the results is also substential higher for the tree models. The results are similar for the fourth dataset. The tree models are again better on average than their linear counterparts, but feature a higher variation.

<center>![](plots/result_oos_regression.png){ width=90% }</center>

## Summary

The results from a Monte Carlo simulation with 100 artificial datasets indicate that XGBoost with tree and linear base learners yield comparable results for classification problems, while tree learners are superior for regression problems. Based on this result, there is no single recommendation which model specification one should use when trying to minimize the model bias. In addition, tree based XGBoost models suffer from higher estimation variance compared to their linear counterparts. This findig is probably related to the more sophisticated parameter space of tree models. The complete code can found on [github](https://github.com/fabianmax/XGBoost_Tree_Linear).
 


## References



