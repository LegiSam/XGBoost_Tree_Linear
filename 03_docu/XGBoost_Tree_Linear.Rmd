---
title: "XGBoost Tree vs. Linear"
author: "Fabian Müller"
date: "17 12 2017"
output: html_document
references:
- id: chen2016
  title: XGBoost - A Scalable Tree Boosting System
  author:
  - family: Chen
    given: Tianqi
  - family: Guestrin
    given: Carlos
  URL: 'https://arxiv.org/pdf/1603.02754.pdf'
  type: article-journal
  issued:
    year: 2016
- id: Natekin2013
  title: Gradient boosting machines, a tutorial
  author:
  - family: Natekin
    given: Alexey
  - family: Knoll
    given: Alois
  URL: 'https://www.frontiersin.org/articles/10.3389/fnbot.2013.00021/full'
  type: article-journal
  issued:
    year: 2013
      
---

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Im Rahmen der diesjährigen [H2O World](https://h2oworld.h2o.ai) fand unter anderem auch ein [Kaggle Grandmaster Panel](https://www.youtube.com/watch?feature=youtu.be&v=bFHRmesTCc0&app=desktop) statt. Die Teilnehmner, [Gilberto Titericz](https://www.kaggle.com/titericz) (Airbnb), Mathias Müller (H2O.ai), [Dmitry Larko](https://www.kaggle.com/dmitrylarko) (H2O.ai), Marios Michailidis (H2O.ai) und Mark Landry (H2O.ai), beantworteten darin verschiedene Fragen rund um das Thema Data Science und Kaggle. 

Eine, zugegebenermaßen zu erwartende, Frage aus dem Publikum drehte sich um die von den Grandmastern genutzten Tools und Algorithmen. Gemeinsamer Nenner war hier die Gradient Boosting Implementation XGBoost [@chen2016]. Eine an sich nicht weiter verwunderliche Antwort, ist doch schon lange bekannt, dass XGBoost einer der am weitesten verbreitete Algorithmus für Data Science Fragestellung ist. 

Die Popularität von XGBoost manifestiert sich unter anderem auch in unzähligen Blogbeiträgen zur Verwendung von XGBoost. Dies beinhaltet Tutorials für [R](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial) und [Python](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn), dem [Tuning der Hyperparameter](https://www.dataiku.com/learn/guide/code/python/advanced-xgboost-tuning.html) und sogar dem Einsatz von [XGBoost auf GPUS mit Hilfe von CUDA](https://devblogs.nvidia.com/parallelforall/gradient-boosting-decision-trees-xgboost-cuda).

Auch wir bei STATWORX setzten XGBoost immer wieder erfolgreich bei unseren externen und internen Projekten ein. Eine Frage die uns dabei immer wieder begegnet ist die nach dem zu verwendenden **Base-Learnern** (auch genannt *Booster*). Im Folgenden soll diese Fragestellung systematischer untersucht werden.

## Weak Learner

Grundsätzlich kann Gradient Boosting als Kombination verschiedener einzelner Modelle (sogenannte *Base Learner* oder *Weak Learner*) zu einem Ensemble Modell interpretiert werden (@Natekin2013). 
Die Wahl der Base Learner ist dabei theoretisch keine Grenzen gesetzt. In der Praxis haben sich verschiedene Methoden bewährt:   
Lineare und penalisierte Modelle (), (P/B-) Splines () und insbesondere Entscheidungsbäume (). Wenig häufig genutzte Base Learner umfassen zum Beispiel Random Effects (Tutz/Groll 2009), Radial Basis Functions (Gomez-Verdejo et al. 2002), Markov Random Fields () und Wavlets (Dubossarsky et al. 2016). 

Chen and Guestrin (2016) beschreiben die in XGBoost umgesetzte Form des Gradient Boosting, gegeben der Daten $D = \{(y_i, x_i)\}$, als additive Funktion der folgenden Form:

$$\hat{y_i} = \phi(x_i) = \sum^{K}_{k=1} f_k(x_i),\textrm{  } f_k\in F$$

Dabei wird im Original $f_k(x) \forall k = 1, ...K$ als *Classification and Regression Tree* (CART) definiert. Aufmerksame Leser der Dokumentation wissen jedoch, dass mit Hilfe des `booster` Arguments eine Möglichkeit gibt die funktionale Form von $f_k(x)$ zu ändern:

```{r, eval=FALSE}
# Beispiel XGBoost mit Tree Booster in R
xgboost(data = train_DMatrix,
        obj = "reg:linear".
        eval_metric = "rmse",
        booster = "gbtree")
```

Zur Auswahl stehen sowohl Entscheidungsbäume (`gbtree` und `dart`) wie auch lineare Modelle (`gblinear`).


## Simulation and Setup

We use the following Monte Carlo simulation:

1) Draw a random number $n$ from a uniform distrubtion $[100, 2500]$.   
2) Simulate four datasets each having $n$ observations, two for classification and two for regression.   
3) On each dataset, train a boosting model with tree and linear base learners, respectively.   
4) Calculate an appropriate error metric for each model on each dataset.   

Repeat the outlined procedure $m = 100$ times. 

As for simulation, we use the functions `twoClassSim()`, `LPH07_1()`, `LPH07_2()`, `SLC14_1()` from the `caret` package. In addition to the relevant features, a variying number of (correlated) random features was added. Note that in order to match real life data, all data generating processes involve non-linear components. For further details, we advise the reader to take a look at the `caret` package documentation.

For each datasets, we apply the same (random) splitting strategies where 70% of the data goes to training, 15% is used for validation, and the last 15% is used for testing. Regarding hyperparameter tuning, we use a grid-search strategy in combindation with 10-fold crossvalidation on the training data. Regardless of the base learner type, $L1$ (`alpha`) and $L2$ (`lambda`) regularization were tuned using a shared parameter space.
For tree boosting, the learning rate (`eta`) was held constant at 0.3 while tuning the optimal tree size (`max_depth`). Finally, we used a fixed number of 1000 boosting iterations (`nrounds`) in combination with ten early stopping rounds (`early_stopping_rounds`) on the validation frame. The final performance was evaluated by appliying the model with the best parameters on the test dataset. 


## Results

Figure 1 shows the distribution of out of sample classification errors for both datasets measured by the AUC.

For the first dataset, the models using tree learners ($\mu_{tree} = 0.952$) are on average better than the models with linear learners ($\mu_{linear} = 0.915$). However, the tree models exhibit a greater variance ($\sigma_{tree} = 0.037$, $\sigma_{linear} = 0.015$) that manifest itself in a bimodal distribution.

For the second dataset, the relationship is reversed. On average, the linear models are slightly better ($\mu_{tree} = 0.733$, $\mu_{linear} = 0.755$) and the tree models exhibit a lower variance ($\sigma_{tree} = 0.032$, $\sigma_{linear} = 0.039$)


<center>![](plots/result_oos_classification.png){ width=90%}</center>

Figure 2 shows the final out of sample regression error for both datasets measured by the RMSE.

In contrast to the classification case, there is for both datasets a substential difference in performance in favor of the tree models.
For the third dataset, the tree models are on average ($\mu_{tree} = 17.423$) better than their linear counterparts (($\mu_{linear} = 45.430$)). Also, the variance of the results is also substential higher for the tree models ($\sigma_{tree} = 8.729$, $\sigma_{linear} = 2.510$).

For the fourth dataset, the results are similar. The tree models ($\mu_{tree} = 5.971$) are again better on average than their linear counterparts (($\mu_{linear} =17.672$)), but feature a higher dispersion ($\sigma_{tree} = 2.733$, $\sigma_{linear} = 0.983$).


<center>![](plots/result_oos_regression.png){ width=90% }</center>


Somewhat surprisingly, there is just a small to medium correlation between the perfomances of the two models for a given dataset (see figure 3):

Dataset| Correlation
-----|-----:
1|0.146
2|0.646
3|0.123
4|0.297


<center>![](plots/result_oos_cor.png){ width=90% }</center>


In the end, it might be worth to take a look at the relationship between model performance and number of obversations in a dataset.
As one can see in figure 4, there is almost no evidence for such an relationship. The performance is indepdentend of the number of simulated data points (at least for the range between 100 and 2500 observations). As described before, figure 3 also illustrates the greater variance of the tree based models.

<center>![](plots/result_oos_n_sims.png){ width=90% }</center>


## Summary

The results from a Monte Carlo simulation with 100 artificial datasets show that XGBoost with tree and linear base learners yield comparable results for classification problems, while tree learners are superior for regression problems. Based on this result, there is no single recommendation which model specification one should when trying to minimize the model bias. In addition, tree based XGBoost models suffer from higher estimation variance compared to their linear counterparts. This findig is probably related to the more sophisticated parameter space of tree models. 










## References



